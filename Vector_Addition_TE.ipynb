{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Working with operators using Tensor Expression: Relay describes a computation as a set of operators, each of these operators cab be represented as TE which is domain-specific."
      ],
      "metadata": {
        "id": "_9OAYW3A5rLB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Writing and Scheduling Vector Addition in TE for CPU:"
      ],
      "metadata": {
        "id": "fw48u4iW5BEF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dy2wULGUXeq5",
        "outputId": "fea90eeb-db9d-4139-e396-70a5bf8ab857"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: apache-tvm in /usr/local/lib/python3.10/dist-packages (0.14.dev273)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (24.2.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (2.2.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (4.4.2)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (0.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (1.26.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (5.9.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (1.13.1)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (6.3.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (4.12.2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "%%shell\n",
        "pip install apache-tvm --pre"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tvm\n",
        "import tvm.testing\n",
        "from tvm import te\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "J2OG8TlfXxYT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tgt = tvm.target.Target(target=\"llvm\", host=\"llvm\")"
      ],
      "metadata": {
        "id": "Iw0TH74VYNvB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = te.var(\"n\") #Defining symbolic variable - represents the dim of tensir\n",
        "A = te.placeholder((n,), name=\"A\") #represents an input tensor, defines the shape(here 1D with n elements each)\n",
        "B = te.placeholder((n,), name=\"B\")\n",
        "C = te.compute(A.shape, lambda i: A[i] + B[i], name=\"C\")\n",
        "#te.compute - creates a computational graph describing how the addition should be performed when the computation is executed.\n",
        "#shape of result tensor, computation rule"
      ],
      "metadata": {
        "id": "BdNZTvP7YSTo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = te.create_schedule(C.op) # creating a default schedule, a schedule defines the computational stategy - how a TE should be executed on hardware."
      ],
      "metadata": {
        "id": "rCnuZPYdbVUr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fadd = tvm.build(s, [A, B, C], tgt, name=\"myadd\") #compiles a TE and its schedule into executable code input(schedule,TE,target,name of func)"
      ],
      "metadata": {
        "id": "T_WVVPaverXF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creates a TVM device object, specifies the type of device, 0 - specifies device index, first available CPU\n",
        "dev = tvm.device(tgt.kind.name, 0)\n",
        "\n",
        "n = 1024\n",
        "#a,b,c are initialied. Created on specified device (dev) ensuring they are accessible for the computation to be run on the CPU.\n",
        "a = tvm.nd.array(np.random.uniform(size=n).astype(A.dtype), dev)# arrays are converted to TVM NDArrays and allocated on the device\n",
        "b = tvm.nd.array(np.random.uniform(size=n).astype(B.dtype), dev)# generates random floating_point numbers btw 0 & 1 of length 1024\n",
        "c = tvm.nd.array(np.zeros(n, dtype=C.dtype), dev)\n",
        "fadd(a, b, c)\n",
        "#compares the elements of TVM computation result (c.numpy) with numpy reference computation\n",
        "tvm.testing.assert_allclose(c.numpy(), a.numpy() + b.numpy())"
      ],
      "metadata": {
        "id": "F8riBizRjf9-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Timing the numpy implementation\n",
        "import timeit\n",
        "\n",
        "np_repeat = 100\n",
        "np_running_time = timeit.timeit(\n",
        "    setup=\"import numpy\\n\"\n",
        "    \"n = 32768\\n\"\n",
        "    'dtype = \"float32\"\\n'\n",
        "    \"a = numpy.random.rand(n, 1).astype(dtype)\\n\"\n",
        "    \"b = numpy.random.rand(n, 1).astype(dtype)\\n\",\n",
        "    stmt=\"answer = a + b\",\n",
        "    number=np_repeat,\n",
        ")\n",
        "print(\"Numpy running time: %f\" % (np_running_time / np_repeat))\n",
        "\n",
        "#Evaluating the TVM implementation\n",
        "def evaluate_addition(func, target, optimization, log):\n",
        "    dev = tvm.device(target.kind.name, 0)\n",
        "    n = 32768\n",
        "    a = tvm.nd.array(np.random.uniform(size=n).astype(A.dtype), dev)\n",
        "    b = tvm.nd.array(np.random.uniform(size=n).astype(B.dtype), dev)\n",
        "    c = tvm.nd.array(np.zeros(n, dtype=C.dtype), dev)\n",
        "\n",
        "    evaluator = func.time_evaluator(func.entry_name, dev, number=10)\n",
        "    mean_time = evaluator(a, b, c).mean                             #runs the evaluation and return the mean time for the addition\n",
        "    print(\"%s: %f\" % (optimization, mean_time))\n",
        "\n",
        "    log.append((optimization, mean_time))\n",
        "\n",
        "\n",
        "log = [(\"numpy\", np_running_time / np_repeat)]\n",
        "evaluate_addition(fadd, tgt, \"naive\", log=log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCs810Ktpo11",
        "outputId": "f16b6e7d-7b05-4cb7-8d60-5e5e4234b77b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numpy running time: 0.000014\n",
            "naive: 0.000008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Updating the Schedule to Use Parallelism"
      ],
      "metadata": {
        "id": "RT00jsXnvD2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s[C].parallel(C.op.axis[0]) #schedule for parallelism - specifies 1st axis of the computation - outer loop"
      ],
      "metadata": {
        "id": "1iO5orOsvG6L"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#how the schedule affects the computation, Lower converting provides a way to inspect the generated IR of the computation.\n",
        "print(tvm.lower(s, [A, B, C], simple_mode=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UJz82HJvJre",
        "outputId": "be00ae49-1ab3-492e-d817-217c403dde5d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# from tvm.script import ir as I\n",
            "# from tvm.script import tir as T\n",
            "\n",
            "@I.ir_module\n",
            "class Module:\n",
            "    @T.prim_func\n",
            "    def main(A: T.handle, B: T.handle, C: T.handle):\n",
            "        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n",
            "        n = T.int32()\n",
            "        A_1 = T.match_buffer(A, (n,), strides=(\"stride\",), buffer_type=\"auto\")\n",
            "        B_1 = T.match_buffer(B, (n,), strides=(\"stride\",), buffer_type=\"auto\")\n",
            "        C_1 = T.match_buffer(C, (n,), strides=(\"stride\",), buffer_type=\"auto\")\n",
            "        for i in T.parallel(n):\n",
            "            C_2 = T.Buffer((C_1.strides[0] * n,), data=C_1.data, buffer_type=\"auto\")\n",
            "            A_2 = T.Buffer((A_1.strides[0] * n,), data=A_1.data, buffer_type=\"auto\")\n",
            "            B_2 = T.Buffer((B_1.strides[0] * n,), data=B_1.data, buffer_type=\"auto\")\n",
            "            C_2[i * C_1.strides[0]] = A_2[i * A_1.strides[0]] + B_2[i * B_1.strides[0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fadd_parallel = tvm.build(s, [A, B, C], tgt, name=\"myadd_parallel\")\n",
        "fadd_parallel(a, b, c)\n",
        "\n",
        "tvm.testing.assert_allclose(c.numpy(), a.numpy() + b.numpy())\n",
        "\n",
        "evaluate_addition(fadd_parallel, tgt, \"parallel\", log=log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8IDHzOavufG",
        "outputId": "aaa74724-c93d-4ae3-dc23-888ee55f5a4e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "parallel: 0.000015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Updating the Schedule to Use Vectorization"
      ],
      "metadata": {
        "id": "brr7eJa7yI-z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorization is a powerful technique that allows CPU to perform SIMD - Single Instruction Multiple Data, enabling multiple data points to be proccessed in parallel with single instruction"
      ],
      "metadata": {
        "id": "esKMf2QK-NSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = te.var(\"n\")\n",
        "A = te.placeholder((n,), name=\"A\")\n",
        "B = te.placeholder((n,), name=\"B\")\n",
        "C = te.compute(A.shape, lambda i: A[i] + B[i], name=\"C\")\n",
        "\n",
        "s = te.create_schedule(C.op)"
      ],
      "metadata": {
        "id": "LaJ1yfwryXZk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "factor = 4 # split factor - number of CPU cores\n",
        "\n",
        "outer, inner = s[C].split(C.op.axis[0], factor=factor)\n",
        "s[C].parallel(outer)\n",
        "s[C].vectorize(inner)\n",
        "\n",
        "fadd_vector = tvm.build(s, [A, B, C], tgt, name=\"myadd_parallel\")\n",
        "\n",
        "evaluate_addition(fadd_vector, tgt, \"vector\", log=log)\n",
        "\n",
        "print(tvm.lower(s, [A, B, C], simple_mode=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "me38k4ksyYxS",
        "outputId": "8d87425b-4e99-49e5-fdf5-678f4c8f1f21"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vector: 0.000113\n",
            "# from tvm.script import ir as I\n",
            "# from tvm.script import tir as T\n",
            "\n",
            "@I.ir_module\n",
            "class Module:\n",
            "    @T.prim_func\n",
            "    def main(A: T.handle, B: T.handle, C: T.handle):\n",
            "        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n",
            "        n = T.int32()\n",
            "        A_1 = T.match_buffer(A, (n,), strides=(\"stride\",), buffer_type=\"auto\")\n",
            "        B_1 = T.match_buffer(B, (n,), strides=(\"stride\",), buffer_type=\"auto\")\n",
            "        C_1 = T.match_buffer(C, (n,), strides=(\"stride\",), buffer_type=\"auto\")\n",
            "        for i_outer in T.parallel((n + 3) // 4):\n",
            "            for i_inner_s in range(4):\n",
            "                if T.likely(i_outer * 4 + i_inner_s < n):\n",
            "                    C_2 = T.Buffer((C_1.strides[0] * n,), data=C_1.data, buffer_type=\"auto\")\n",
            "                    A_2 = T.Buffer((A_1.strides[0] * n,), data=A_1.data, buffer_type=\"auto\")\n",
            "                    B_2 = T.Buffer((B_1.strides[0] * n,), data=B_1.data, buffer_type=\"auto\")\n",
            "                    cse_var_1: T.int32 = i_outer * 4 + i_inner_s\n",
            "                    C_2[cse_var_1 * C_1.strides[0]] = A_2[cse_var_1 * A_1.strides[0]] + B_2[cse_var_1 * B_1.strides[0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing the Different Schedules"
      ],
      "metadata": {
        "id": "U6HLEPvmyhoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "baseline = log[0][1]\n",
        "print(\"%s\\t%s\\t%s\" % (\"Operator\".rjust(20), \"Timing\".rjust(20), \"Performance\".rjust(20)))\n",
        "for result in log:\n",
        "    print(\n",
        "        \"%s\\t%s\\t%s\"\n",
        "        % (result[0].rjust(20), str(result[1]).rjust(20), str(result[1] / baseline).rjust(20))\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3F_63o-wydIa",
        "outputId": "b724b8e0-d5b4-425e-9dac-c8c5ce529b58"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Operator\t              Timing\t         Performance\n",
            "               numpy\t1.3779510002223106e-05\t                 1.0\n",
            "               naive\t          8.3261e-06\t  0.6042377413026091\n",
            "            parallel\t         1.49822e-05\t  1.0872810424741417\n",
            "              vector\t0.00011327570000000001\t   8.220589845482515\n"
          ]
        }
      ]
    }
  ]
}